{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f381a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ec66d25",
   "metadata": {},
   "source": [
    "## MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8ce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.mcp import MCPServerStdio\n",
    "\n",
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    fetch_tools = await server.list_tools()\n",
    "\n",
    "fetch_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.mcp import MCPServerStdio\n",
    "\n",
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"pubmedmcp@latest\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    pubmed_tools = await server.list_tools()\n",
    "\n",
    "pubmed_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbecff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import Runner, trace, function_tool, Agent, OpenAIChatCompletionsModel, SQLiteSession\n",
    "from openai import AsyncOpenAI\n",
    "from agents.mcp import MCPServerStdio\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "agent_instructions = \"\"\"You are a helpful assistant for Bioinformatics research. \n",
    "You are equipped with pubmed mcp server. You can help users find relevant scientific articles, summarize research papers, and provide insights on various bioinformatics topics.\n",
    "\"\"\"\n",
    "\n",
    "# Define LLM model\n",
    "\n",
    "# model = \"gpt-4.1-mini\"\n",
    "\n",
    "# Define the OpenAI client pointing to a local LLM server\n",
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Define model using the local LLM server client\n",
    "# use gpt-oss which is better for tool usage and reasoning even though it takes time\n",
    "model = OpenAIChatCompletionsModel(model = \"gpt-oss\",openai_client= client)\n",
    "\n",
    "\n",
    "\n",
    "# Define a session to maintain context across interactions\n",
    "session = SQLiteSession(\"pubmed_agent_session.db\")\n",
    "\n",
    "\n",
    "fetch_params = {\"command\": \"uvx\", \"args\": [\"pubmedmcp@latest\"]}\n",
    "\n",
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    pubmed_tools = await server.list_tools()\n",
    "\n",
    "    bioinfo_agent = Agent(name=\"BioinformaticsAgent\",\n",
    "                    instructions=agent_instructions,\n",
    "                    model=model,\n",
    "                    mcp_servers=[server])\n",
    "    \n",
    "    #query = \"Find recent articles on CRISPR gene editing in plants and summarize the key findings.\"\n",
    "    query=\"Find 5 review articles on latest use of AI in genomics. Provide urls of the articles in markdown format.\"\n",
    "    \n",
    "    result = await Runner.run(bioinfo_agent, query, session=session)\n",
    "    print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b9bf8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/blocks.py\", line 1586, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/utils.py\", line 1015, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 541, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 900, in _submit_fn\n",
      "    response = await self.fn(*inputs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1478256/1668370752.py\", line 10, in chat\n",
      "    result = await Runner.run(bioinfo_agent, message, session=session)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/run.py\", line 358, in run\n",
      "    return await runner.run(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/run.py\", line 579, in run\n",
      "    all_tools = await AgentRunner._get_all_tools(current_agent, context_wrapper)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/run.py\", line 1873, in _get_all_tools\n",
      "    return await agent.get_all_tools(context_wrapper)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/agent.py\", line 113, in get_all_tools\n",
      "    mcp_tools = await self.get_mcp_tools(run_context)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/agent.py\", line 107, in get_mcp_tools\n",
      "    return await MCPUtil.get_all_function_tools(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/mcp/util.py\", line 124, in get_all_function_tools\n",
      "    server_tools = await cls.get_function_tools(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/mcp/util.py\", line 149, in get_function_tools\n",
      "    tools = await server.list_tools(run_context, agent)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/c/Users/PriyabrataPanigrahi/Downloads/ai/aienv/lib/python3.12/site-packages/agents/mcp/server.py\", line 298, in list_tools\n",
      "    raise UserError(\"Server not initialized. Make sure you call `connect()` first.\")\n",
      "agents.exceptions.UserError: Server not initialized. Make sure you call `connect()` first.\n",
      "[non-fatal] Tracing client error 400: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Invalid type for 'data[1].span_data.result': expected an array of strings, but got null instead.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": \"data[1].span_data.result\",\n",
      "    \"code\": \"invalid_type\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async with MCPServerStdio(params=fetch_params, client_session_timeout_seconds=60) as server:\n",
    "    pubmed_tools = await server.list_tools()\n",
    "\n",
    "    bioinfo_agent = Agent(name=\"BioinformaticsAgent\",\n",
    "                    instructions=agent_instructions,\n",
    "                    model=model,\n",
    "                    mcp_servers=[server])\n",
    "    \n",
    "    async def chat(message, history):\n",
    "        result = await Runner.run(bioinfo_agent, message, session=session)\n",
    "        return result.final_output\n",
    "\n",
    "    import gradio as gr\n",
    "    gr.ChatInterface(\n",
    "        chat,\n",
    "        title=\"Clinical Expert Chatbot\",\n",
    "        description=\"Chat with the Clinical Expert\"\n",
    "    ).launch()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
