{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9aa095",
   "metadata": {},
   "source": [
    "## Using Local Ollama model\n",
    "In past we used Open AI model which is paid one. You can try it with 5$ free tier. The limitations is number of token and API rate limit. \n",
    "\n",
    "Alternatively you can install Ollama, pull ollama model and use it locally. \n",
    "\n",
    "In this session, we will explore using Ollama model.\n",
    "\n",
    "Before that, download and install ollama by following https://ollama.com/download\n",
    "\n",
    "Ollama models: https://ollama.com/search \n",
    "\n",
    "- gpt-oss: https://ollama.com/library/gpt-oss \n",
    "- llama3.2: https://ollama.com/library/llama3.2 \n",
    "- deepseek-r1: https://ollama.com/library/deepseek-r1\n",
    "\n",
    "Pull below models by \n",
    "\n",
    "ollama pull [model name] or\n",
    "\n",
    "ollama run [model name]\n",
    "\n",
    "- llama3.2:latest\n",
    "- gpt-oss:latest\n",
    "- deepseek-r1:latest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f548273",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a514f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use local ollama model, we need to import AsyncOpenAI and OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, SQLiteSession\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92137a19",
   "metadata": {},
   "source": [
    "#### Load openai api key from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed328fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9ad6c",
   "metadata": {},
   "source": [
    "#### Define OpenAI client and llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac526ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OpenAI client pointing to a local LLM server\n",
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Define model using client\n",
    "model_name = \"llama3.2:latest\"\n",
    "# model_name = \"gpt-oss\"\n",
    "\n",
    "# gpt-oss is a thinking model. at least 13GB free RAM is required to run it locally. \n",
    "model = OpenAIChatCompletionsModel(model = model_name,openai_client= client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263a506",
   "metadata": {},
   "source": [
    "#### Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent using the local model\n",
    "local_agent = Agent(name=\"Bioinfo Local Agent\",\n",
    "                    instructions=\"You are a helpful bioinformatics assistant using local LLM server.\",\n",
    "                    model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749177b",
   "metadata": {},
   "source": [
    "#### Run Agent as chat interface with session enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc775ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SQLiteSession(\"bioinfo_session\")\n",
    "\n",
    "async def chat(message, history):\n",
    "    result = await Runner.run(local_agent, message, session=session)\n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "gr.ChatInterface(\n",
    "    chat,\n",
    "    title=\"Bioinformatics Expert Chatbot\",\n",
    "    description=\"Chat with the Bioinformatics Expert\"\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
