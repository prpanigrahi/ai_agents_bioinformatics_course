{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9aa095",
   "metadata": {},
   "source": [
    "## Using Local Ollama model\n",
    "In past we used Open AI model which is paid one. You can try it with 5$ free tier. The limitations is number of token and API rate limit. \n",
    "\n",
    "Alternatively you can install Ollama, pull ollama model and use it locally. \n",
    "\n",
    "In this session, we will explore using Ollama model.\n",
    "\n",
    "Before that, download and install ollama by following https://ollama.com/download\n",
    "\n",
    "Ollama models: https://ollama.com/search \n",
    "\n",
    "- gpt-oss: https://ollama.com/library/gpt-oss \n",
    "- llama3.2: https://ollama.com/library/llama3.2 \n",
    "- deepseek-r1: https://ollama.com/library/deepseek-r1\n",
    "\n",
    "Pull below models by \n",
    "\n",
    "ollama pull [model name] or\n",
    "\n",
    "ollama run [model name]\n",
    "\n",
    "- llama3.2:latest\n",
    "- gpt-oss:latest\n",
    "- deepseek-r1:latest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f548273",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a514f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use local ollama model, we need to import AsyncOpenAI and OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, OpenAIChatCompletionsModel, SQLiteSession\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92137a19",
   "metadata": {},
   "source": [
    "#### Load openai api key from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9ad6c",
   "metadata": {},
   "source": [
    "#### Define OpenAI client and llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac526ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OpenAI client pointing to a local LLM server\n",
    "client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Define model using client\n",
    "model_name = \"llama3.2:latest\"\n",
    "# model_name = \"gpt-oss\"\n",
    "\n",
    "# gpt-oss is a thinking model. at least 13GB free RAM is required to run it locally. \n",
    "model = OpenAIChatCompletionsModel(model = model_name,openai_client= client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263a506",
   "metadata": {},
   "source": [
    "#### Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent using the local model\n",
    "local_agent = Agent(name=\"Bioinfo Local Agent\",\n",
    "                    instructions=\"You are a helpful bioinformatics assistant using local LLM server.\",\n",
    "                    model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749177b",
   "metadata": {},
   "source": [
    "#### Run Agent as chat interface with session enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc775ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SQLiteSession(\"bioinfo_session\")\n",
    "\n",
    "async def chat(message, history):\n",
    "    result = await Runner.run(local_agent, message, session=session)\n",
    "    return result.final_output\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "gr.ChatInterface(\n",
    "    chat,\n",
    "    title=\"Bioinformatics Expert Chatbot\",\n",
    "    description=\"Chat with the Bioinformatics Expert\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e9b49",
   "metadata": {},
   "source": [
    "## Use models of other LLM providers\n",
    "\n",
    "The process is same as using Local LLM as shown above. Only urls are different. \n",
    "\n",
    "We must have API key setup for it work and you may be in their paid plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c72058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base URLs for Gemini and Deepseek APIs.\n",
    "# These urls are compatible with OpenAI API endpoints.\n",
    "# So openAI sdk agent can be used to call these models\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "from agents import OpenAIChatCompletionsModel\n",
    "import os\n",
    "\n",
    "# Initialize AsyncOpenAI clients for Deepseek and Gemini using the base URLs\n",
    "# and the respective API keys from environment variables.\n",
    "# Client helps to make calls to the models\n",
    "deepseek_client = AsyncOpenAI(base_url=DEEPSEEK_BASE_URL, api_key=os.getenv(\"DEEPSEEK_API_KEY\"))\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Create OpenAIChatCompletionsModel instances for Deepseek and Gemini models\n",
    "# OpenAIChatCompletionsModel allows using these models with the agent framework\n",
    "deepseek_model = OpenAIChatCompletionsModel(model=\"deepseek-chat\", openai_client=deepseek_client)\n",
    "gemini_model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=gemini_client)\n",
    "\n",
    "instructions = \"\"\"You are an expert in Bioinformatics domain.\n",
    "Provide detailed and accurate explanations to user queries.\"\"\"\n",
    "\n",
    "deepseek_agent = Agent(name=\"Explainer\",\n",
    "              instructions=instructions,\n",
    "                model=deepseek_model)\n",
    "\n",
    "gemini_agent = Agent(name=\"Explainer\",\n",
    "              instructions=instructions,\n",
    "                model=gemini_model)\n",
    "\n",
    "query = \"Tell me about ACMG classification of genetic variants\"\n",
    "\n",
    "result_deepseek = await Runner.run(deepseek_agent, query)\n",
    "result_gemini = await Runner.run(gemini_agent, query)\n",
    "\n",
    "print(\"Deepseek Agent Response:\\n\", result_deepseek.final_output)\n",
    "print(\"\\nGemini Agent Response:\\n\", result_gemini.final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
